# awesome-self-consuming-ai-papers
Papers about self-consuming generative models, or "MAD AI".

Updated as of 5/14/2024.

"solving" or analyzing madcow:


Self-Correcting Self-Consuming Loops for Generative Model Training

On the Stability of Iterative Retraining of Generative Models on their own Data

Model Collapse Demystified: The Case of Regression

A Tale of Tails: Model Collapse as a Change of Scaling Laws (same authors as Model Collapse Demystified)

Towards Theoretical Understandings of Self-Consuming Generative Models

Is Model Collapse Inevitable? Breaking the Curse of Recursion by Accumulating Real and Synthetic Data (Donoho's paper)

Heat Death of Generative Models in Closed-Loop Learning


additional madcow experiments/observations (e.g., LLMs):

Large Language Models Suffer From Their Own Output: An Analysis of the Self-Consuming Training Loop

The Curious Decline of Linguistic Diversity: Training Language Models on Synthetic Text

Nepotistically Trained Generative-AI Models Collapse

AI and the Problem of Knowledge Collapse

ethics / societal implications of madcow:

Are large language models a threat to digital public goods? evidence from activity on stack overflow

Generative Artificial Intelligence Enhances Creativity but Reduces the Diversity of Novel Content

LLMs may dominate information access: Neural retrievers are biased towards LLM-generated texts


"using" madcow for "good" (i.e., training generative models on AI-generated or AI-curated data - this cluster might be too big for a workshop, but it is important):

Bootstrapping LLM-based Task-Oriented Dialogue Agents via Self-Talk

Iterated Denoising Energy Matching for Sampling from Boltzmann Densities

Upgrading VAE Training With Unlimited Data Plans Provided by Diffusion Models

How to Train Data-Efficient LLMs
